import streamlit as st
import pandas as pd
import numpy as np
import joblib
import os
import json
from datetime import datetime
from utils.load_data import load_csv
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="D·ª± ƒëo√°n gi√° nh√†",
    page_icon="üè†",
    layout="wide"
)

# C·∫•u h√¨nh debug
DEBUG = False  # Set to True to show debug information

# Ti√™u ƒë·ªÅ
st.title("üè† D·ª± ƒëo√°n gi√° nh√† ƒë·∫•t")
st.markdown("---")

# H√†m debug print
def debug_print(message: str, data=None):
    """In th√¥ng tin debug n·∫øu DEBUG=True"""
    if DEBUG:
        st.write(message)
        if data is not None:
            st.write(data)

# H√†m chuy·ªÉn ƒë·ªïi gi√° tr·ªã LegalStatus
def convert_legal_status_to_label(status: str) -> int:
    """
    Chuy·ªÉn ƒë·ªïi gi√° tr·ªã LegalStatus t·ª´ text sang s·ªë theo label encoding
    """
    status_map = {
        "C√≥": 1,
        "Kh√¥ng": 0
    }
    return status_map.get(status, 0)

# H√†m chuy·ªÉn ƒë·ªïi gi√° tr·ªã Furnishing
def convert_furnishing_to_label(furnishing: str) -> int:
    """
    Chuy·ªÉn ƒë·ªïi gi√° tr·ªã Furnishing t·ª´ text sang s·ªë theo label encoding
    """
    furnishing_map = {
        "C√≥": 1,
        "Kh√¥ng": 0
    }
    return furnishing_map.get(furnishing, 0)

# H√†m chu·∫©n h√≥a ph√¢n ph·ªëi
def normalize_distribution(df: pd.DataFrame) -> pd.DataFrame:
    """
    √Åp d·ª•ng log1p cho c√°c c·ªôt l·ªách ph√¢n ph·ªëi
    """
    # √Åp d·ª•ng log1p cho c√°c c·ªôt l·ªách ph√¢n ph·ªëi
    for col in ['Area', 'Bedrooms', 'Bathrooms', 'Floors', 'AccessWidth', 'FacadeWidth']:
        df[col] = np.log1p(df[col])
    return df

# H√†m x·ª≠ l√Ω ƒë·∫∑c tr∆∞ng cho Linear Regression
def process_linear_regression_features(df: pd.DataFrame, scalers: dict) -> pd.DataFrame:
    """X·ª≠ l√Ω ƒë·∫∑c tr∆∞ng cho Linear Regression"""
    df_filtered = df.copy()
    debug_print("Linear Regression - Input data:", df_filtered)
    
    # Chu·∫©n h√≥a bi·∫øn s·ªë
    numerical_columns = ['Area', 'Bedrooms', 'Bathrooms', 'Floors', 
                        'AccessWidth', 'FacadeWidth', 'Distribute', 'GDP_USD']
    for col in numerical_columns:
        if col in df_filtered.columns and col in scalers:
            df_filtered[col] = scalers[col].transform(df_filtered[[col]])
            debug_print(f"Linear Regression - After scaling {col}:", df_filtered[col].values)
    
    # M√£ h√≥a bi·∫øn ph√¢n lo·∫°i
    categorical_columns = ['LegalStatus', 'Furnishing']
    for col in categorical_columns:
        if col in df_filtered.columns:
            # Chuy·ªÉn ƒë·ªïi gi√° tr·ªã s·ªë th√†nh 'yes'/'no'
            df_filtered[col] = df_filtered[col].map({1: 'yes', 0: 'no'})
            dummies = pd.get_dummies(df_filtered[col], prefix=col)
            df_filtered = pd.concat([df_filtered.drop(col, axis=1), dummies], axis=1)
            debug_print(f"Linear Regression - After encoding {col}:", df_filtered)
    
    # ƒê·∫£m b·∫£o th·ª© t·ª± c·ªôt
    expected_columns = [
        'Area', 'Bedrooms', 'Bathrooms', 'Floors', 'AccessWidth', 
        'FacadeWidth', 'Distribute', 'GDP_USD',
        'LegalStatus_no', 'LegalStatus_yes',
        'Furnishing_no', 'Furnishing_yes'
    ]
    
    # ƒê·∫£m b·∫£o t·∫•t c·∫£ c√°c c·ªôt c·∫ßn thi·∫øt ƒë·ªÅu c√≥ m·∫∑t
    for col in expected_columns:
        if col not in df_filtered.columns:
            df_filtered[col] = 0
    
    df_filtered = df_filtered[expected_columns]
    debug_print("Linear Regression - Final processed data:", df_filtered)
    return df_filtered

# H√†m x·ª≠ l√Ω ƒë·∫∑c tr∆∞ng cho KNN
def process_knn_features(df: pd.DataFrame, scalers: dict) -> pd.DataFrame:
    """X·ª≠ l√Ω ƒë·∫∑c tr∆∞ng cho KNN"""
    df_filtered = df.copy()
    debug_print("KNN - Input data:", df_filtered)
    
    # Chu·∫©n h√≥a bi·∫øn s·ªë
    numerical_columns = ['Area', 'Bedrooms', 'Bathrooms', 'Floors', 
                        'AccessWidth', 'FacadeWidth', 'Distribute', 'GDP_USD']
    for col in numerical_columns:
        if col in df_filtered.columns and col in scalers:
            df_filtered[col] = scalers[col].transform(df_filtered[[col]])
            debug_print(f"KNN - After scaling {col}:", df_filtered[col].values)
    
    # M√£ h√≥a bi·∫øn ph√¢n lo·∫°i
    categorical_columns = ['LegalStatus', 'Furnishing']
    for col in categorical_columns:
        if col in df_filtered.columns:
            # Chuy·ªÉn ƒë·ªïi gi√° tr·ªã s·ªë th√†nh 'yes'/'no'
            df_filtered[col] = df_filtered[col].map({1: 'yes', 0: 'no'})
            dummies = pd.get_dummies(df_filtered[col], prefix=col)
            df_filtered = pd.concat([df_filtered.drop(col, axis=1), dummies], axis=1)
            debug_print(f"KNN - After encoding {col}:", df_filtered)
    
    # ƒê·∫£m b·∫£o th·ª© t·ª± c·ªôt
    expected_columns = [
        'Area', 'Bedrooms', 'Bathrooms', 'Floors', 'AccessWidth', 
        'FacadeWidth', 'Distribute', 'GDP_USD',
        'LegalStatus_no', 'LegalStatus_yes',
        'Furnishing_no', 'Furnishing_yes'
    ]
    
    # ƒê·∫£m b·∫£o t·∫•t c·∫£ c√°c c·ªôt c·∫ßn thi·∫øt ƒë·ªÅu c√≥ m·∫∑t
    for col in expected_columns:
        if col not in df_filtered.columns:
            df_filtered[col] = 0
    
    df_filtered = df_filtered[expected_columns]
    debug_print("KNN - Final processed data:", df_filtered)
    return df_filtered

# H√†m x·ª≠ l√Ω ƒë·∫∑c tr∆∞ng cho XGBoost
def process_xgboost_features(df: pd.DataFrame, scalers: dict) -> pd.DataFrame:
    """X·ª≠ l√Ω ƒë·∫∑c tr∆∞ng cho XGBoost"""
    df_filtered = df.copy()
    debug_print("XGBoost - Input data:", df_filtered)
    
    # Chu·∫©n h√≥a bi·∫øn s·ªë
    numerical_columns = ['Area', 'Bedrooms', 'Bathrooms', 'Floors', 
                        'AccessWidth', 'FacadeWidth', 'Distribute', 'GDP_USD']
    for col in numerical_columns:
        if col in df_filtered.columns and col in scalers:
            df_filtered[col] = scalers[col].transform(df_filtered[[col]])
            debug_print(f"XGBoost - After scaling {col}:", df_filtered[col].values)
    
    # M√£ h√≥a bi·∫øn ph√¢n lo·∫°i
    categorical_columns = ['LegalStatus', 'Furnishing']
    for col in categorical_columns:
        if col in df_filtered.columns:
            df_filtered[col] = df_filtered[col].astype('category').cat.codes
            debug_print(f"XGBoost - After encoding {col}:", df_filtered[col].values)
    
    # ƒê·∫£m b·∫£o th·ª© t·ª± c·ªôt
    expected_columns = [
        'Area', 'Bedrooms', 'Bathrooms', 'Floors', 'AccessWidth', 
        'FacadeWidth', 'LegalStatus', 'Furnishing', 'Distribute', 'GDP_USD'
    ]
    
    df_filtered = df_filtered[expected_columns]
    debug_print("XGBoost - Final processed data:", df_filtered)
    return df_filtered

# Load d·ªØ li·ªáu ƒë·ªãa l√Ω
@st.cache_resource
def load_location_data():
    base_dir = os.path.dirname(os.path.abspath(__file__))
    wiki_path = os.path.join(base_dir, 'data', 'raw', 'wiki.json')
    gdp_path = os.path.join(base_dir, 'data', 'raw', 'gdp_provinces.csv')
    
    if not os.path.exists(wiki_path):
        st.error(f"Kh√¥ng t√¨m th·∫•y file wiki.json t·∫°i: {wiki_path}")
        return {}
    
    # Load d·ªØ li·ªáu wiki
    with open(wiki_path, 'r', encoding='utf-8') as f:
        location_data = json.load(f)
    
    # Load v√† merge GDP data
    if os.path.exists(gdp_path):
        try:
            # Load GDP data
            gdp_data = pd.read_csv(gdp_path)
            debug_print("GDP Data loaded:", gdp_data.head())  # Debug info
            
            # ƒê·ªïi t√™n c·ªôt cho ph√π h·ª£p
            gdp_data = gdp_data.rename(columns={
                'T√™n t·ªânh, th√†nh ph·ªë': 'Province',
                'T·ªïng GRDP (t·ªâ USD)': 'GDP_USD'
            })
            
            # Chuy·ªÉn t√™n t·ªânh/th√†nh ph·ªë trong GDP data v·ªÅ ch·ªØ th∆∞·ªùng
            gdp_data['Province'] = gdp_data['Province'].str.lower()
            
            # Debug info
            debug_print("GDP Data after renaming:", gdp_data.head())
            debug_print("Available provinces in GDP data:", gdp_data['Province'].unique())
            debug_print("Available provinces in location data:", list(location_data.keys()))
            
            # Merge GDP data v·ªõi location data
            for province in location_data:
                province_gdp = gdp_data[gdp_data['Province'] == province.lower()]
                if not province_gdp.empty:
                    gdp_value = float(province_gdp['GDP_USD'].values[0])
                    debug_print(f"Found GDP for {province}: {gdp_value}")  # Debug info
                    for district in location_data[province]:
                        location_data[province][district]['gdp_usd'] = gdp_value
                else:
                    st.warning(f"Kh√¥ng t√¨m th·∫•y GDP cho t·ªânh/th√†nh ph·ªë: {province}")
        except Exception as e:
            st.error(f"L·ªói khi x·ª≠ l√Ω GDP data: {str(e)}")
            st.error("Chi ti·∫øt l·ªói:")
            st.error(str(e.__class__.__name__))
            st.error(str(e))
    else:
        st.error(f"Kh√¥ng t√¨m th·∫•y file gdp_provinces.csv t·∫°i: {gdp_path}")
    
    return location_data

# Load model v√† scaler
@st.cache_resource
def load_models_and_scalers():
    base_dir = os.path.dirname(os.path.abspath(__file__))
    models_dir = os.path.join(base_dir, 'models')
    
    if not os.path.exists(models_dir):
        st.error(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c models t·∫°i: {models_dir}")
        return {}, {}
    
    # Dictionary ƒë·ªÉ l∆∞u c√°c model
    models = {}
    
    # Th√¥ng tin v·ªÅ c√°c model
    model_info = {
        'linear_regression_model.joblib': {
            'name': 'Linear Regression',
            'scaler_file': 'scalers_linear_regression.joblib',
            'description': 'Linear Regression (R2: 0.518)'
        },
        'knn_model.joblib': {
            'name': 'KNN',
            'scaler_file': 'scalers_knn.joblib',
            'description': 'KNN (R2: 0.612)'
        },
        'xgboost_model.joblib': {
            'name': 'XGBoost',
            'scaler_file': 'scalers_xgboost.joblib',
            'description': 'XGBoost (R2: 0.781)'
        }
    }
    
    # Load t·∫•t c·∫£ c√°c model v√† scaler
    for model_file, info in model_info.items():
        model_path = os.path.join(models_dir, model_file)
        scaler_path = os.path.join(models_dir, info['scaler_file'])
        
        if os.path.exists(model_path) and os.path.exists(scaler_path):
            try:
                model = joblib.load(model_path)
                scalers = joblib.load(scaler_path)
                models[info['name']] = {
                    'model': model,
                    'scalers': scalers,
                    'description': info['description']
                }
            except Exception as e:
                st.warning(f"Kh√¥ng th·ªÉ t·∫£i model ho·∫∑c scaler cho {info['name']}: {str(e)}")
    
    return models

try:
    models = load_models_and_scalers()
    location_data = load_location_data()
    if not models:
        st.error("Kh√¥ng t√¨m th·∫•y model n√†o ƒë·ªÉ d·ª± ƒëo√°n.")
        st.stop()
except Exception as e:
    st.error("Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh ho·∫∑c d·ªØ li·ªáu ƒë·ªãa l√Ω. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n v√† file.")
    st.stop()

# Ch·ªçn model
st.subheader("Ch·ªçn m√¥ h√¨nh d·ª± ƒëo√°n")
model_names = list(models.keys())
selected_model_name = st.selectbox(
    "M√¥ h√¨nh",
    model_names,
    index=0,
    help="Ch·ªçn m√¥ h√¨nh ƒë·ªÉ d·ª± ƒëo√°n gi√° nh√†"
)

# Hi·ªÉn th·ªã th√¥ng tin v·ªÅ model ƒë∆∞·ª£c ch·ªçn
if selected_model_name:
    model_info = models[selected_model_name]
    st.info(f"""
    **Th√¥ng tin m√¥ h√¨nh:**
    - {model_info['description']}
 
    """)

# Ch·ªçn ƒë·ªãa ch·ªâ
st.subheader("ƒê·ªãa ch·ªâ")
if not location_data:
    st.error("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªãa l√Ω. Vui l√≤ng ki·ªÉm tra l·∫°i file wiki.json")
    st.stop()

provinces = list(location_data.keys())
selected_province = st.selectbox(
    "T·ªânh/Th√†nh ph·ªë",
    provinces,
    help="Ch·ªçn t·ªânh/th√†nh ph·ªë ƒë·ªÉ xem danh s√°ch qu·∫≠n/huy·ªán v√† th√¥ng tin chi ti·∫øt"
)

districts = list(location_data[selected_province].keys())
selected_district = st.selectbox(
    "Qu·∫≠n/Huy·ªán",
    districts,
    help="Th√¥ng tin s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t t·ª± ƒë·ªông khi ch·ªçn qu·∫≠n/huy·ªán"
)

# L·∫•y th√¥ng tin v·ªÅ qu·∫≠n/huy·ªán ƒë∆∞·ª£c ch·ªçn
district_info = location_data[selected_province][selected_district]
commune_density = float(district_info['distribute'].replace('.', ''))
commune_count = int(district_info['communes'])

# Hi·ªÉn th·ªã th√¥ng tin v·ªÅ qu·∫≠n/huy·ªán
st.info(f"""
**Th√¥ng tin {selected_district.title()}:**
- M·∫≠t ƒë·ªô d√¢n s·ªë: {commune_density:,.0f} ng∆∞·ªùi/km¬≤
- S·ªë ph∆∞·ªùng/x√£: {commune_count}
- Di·ªán t√≠ch: {district_info['area']} km¬≤
- D√¢n s·ªë: {district_info['number_people']} ng∆∞·ªùi
""")

# T·∫°o form nh·∫≠p li·ªáu
with st.form("prediction_form"):
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Th√¥ng tin c∆° b·∫£n")
        area = st.number_input("Di·ªán t√≠ch (m¬≤)", min_value=0.0, value=100.0)
        bedrooms = st.number_input("S·ªë ph√≤ng ng·ªß", min_value=0, value=2)
        bathrooms = st.number_input("S·ªë ph√≤ng t·∫Øm", min_value=0, value=1)
        floors = st.number_input("S·ªë t·∫ßng", min_value=0, value=1)
        
    with col2:
        st.subheader("Th√¥ng tin b·ªï sung")
        access_width = st.number_input("ƒê∆∞·ªùng v√†o (m)", min_value=0.0, value=4.0)
        facade_width = st.number_input("M·∫∑t ti·ªÅn (m)", min_value=0.0, value=5.0)
        
    st.subheader("Tr·∫°ng th√°i")
    col3, col4 = st.columns(2)
    
    with col3:
        legal_status = st.radio("Ph√°p l√Ω", ["C√≥", "Kh√¥ng"])
        
    with col4:
        furnishing = st.radio("N·ªôi th·∫•t", ["C√≥", "Kh√¥ng"])
    
    submit_button = st.form_submit_button("D·ª± ƒëo√°n gi√°")

# X·ª≠ l√Ω d·ª± ƒëo√°n
if submit_button:
    try:
        # Debug info tr∆∞·ªõc khi t·∫°o input data
        debug_print("Selected province:", selected_province)
        debug_print("Selected district:", selected_district)
        debug_print("District info before processing:", location_data[selected_province][selected_district])
        
        # T·∫°o DataFrame t·ª´ input
        gdp_value = location_data[selected_province][selected_district].get('gdp_usd', 0)
        debug_print("GDP value to be used:", gdp_value)
        
        input_data = pd.DataFrame({
            'Area': [area],
            'Bedrooms': [bedrooms],
            'Bathrooms': [bathrooms],
            'Floors': [floors],
            'AccessWidth': [access_width],
            'FacadeWidth': [facade_width],
            'LegalStatus': [convert_legal_status_to_label(legal_status)],
            'Furnishing': [convert_furnishing_to_label(furnishing)],
            'Distribute': [commune_density],
            'GDP_USD': [gdp_value]
        })
        
        # Debug info
        debug_print("Input data before processing:", input_data)
        
        # L·∫•y model v√† scaler t∆∞∆°ng ·ª©ng
        model_info = models[selected_model_name]
        model = model_info['model']
        scalers = model_info['scalers']
        
        # √Åp d·ª•ng log1p transformation cho c√°c c·ªôt s·ªë
        input_data = normalize_distribution(input_data)
        debug_print("After log1p transformation:", input_data)
        
        # X·ª≠ l√Ω ƒë·∫∑c tr∆∞ng theo t·ª´ng thu·∫≠t to√°n
        if selected_model_name == 'Linear Regression':
            input_data = process_linear_regression_features(input_data, scalers)
        elif selected_model_name == 'KNN':
            input_data = process_knn_features(input_data, scalers)
        elif selected_model_name == 'XGBoost':
            input_data = process_xgboost_features(input_data, scalers)
        
        # Debug info
        debug_print("Final processed data:", input_data)
        
        # D·ª± ƒëo√°n
        prediction = model.predict(input_data)[0]
        
        # Chuy·ªÉn ƒë·ªïi gi√° tr·ªã d·ª± ƒëo√°n t·ª´ log scale v·ªÅ gi√° tr·ªã th·ª±c
        prediction = np.expm1(prediction)
        
        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        st.markdown("---")
        st.subheader("K·∫øt qu·∫£ d·ª± ƒëo√°n")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric("Gi√° d·ª± ƒëo√°n", f"{prediction:,.2f} t·ª∑ ƒë·ªìng")
        
        with col2:
            st.metric("Gi√°/m¬≤", f"{(prediction * 1000 / area):,.0f} tri·ªáu ƒë·ªìng/m¬≤")
        
        # with col3:
        #     st.metric("Th·ªùi gian d·ª± ƒëo√°n", datetime.now().strftime("%H:%M:%S"))
        
        # Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt
        st.markdown("### Th√¥ng tin chi ti·∫øt")
        st.markdown(f"""
        - M√¥ h√¨nh s·ª≠ d·ª•ng: {selected_model_name}
        - ƒê·ªãa ch·ªâ: {selected_district.title()}, {selected_province.title()}
        - Di·ªán t√≠ch: {area:,.1f} m¬≤
        - S·ªë ph√≤ng ng·ªß: {bedrooms}
        - S·ªë ph√≤ng t·∫Øm: {bathrooms}
        - S·ªë t·∫ßng: {floors}
        - ƒê∆∞·ªùng v√†o: {access_width:,.1f} m
        - M·∫∑t ti·ªÅn: {facade_width:,.1f} m
        - M·∫≠t ƒë·ªô d√¢n s·ªë: {commune_density:,.0f} ng∆∞·ªùi/km¬≤
        - GDP (USD): {gdp_value:,.2f} t·ª∑ USD
        - Ph√°p l√Ω: {legal_status}
        - N·ªôi th·∫•t: {furnishing}
        """)
        
    except Exception as e:
        st.error(f"C√≥ l·ªói x·∫£y ra khi d·ª± ƒëo√°n: {str(e)}")
        st.error("Chi ti·∫øt l·ªói:")
        st.error(str(e.__class__.__name__))
        st.error(str(e))

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center'>
    <p>ƒê∆∞·ª£c ph√°t tri·ªÉn b·ªüi HM&KPDL Team</p>
    <p>¬© 2024 - Phi√™n b·∫£n 1.0</p>
</div>
""", unsafe_allow_html=True) 